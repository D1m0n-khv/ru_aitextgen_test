{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Установим модуль https://docs.aitextgen.io/\n",
    "!pip install aitextgen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aitextgen.TokenDataset import TokenDataset\n",
    "from aitextgen.tokenizers import train_tokenizer\n",
    "from aitextgen.utils import GPT2ConfigCPU\n",
    "from aitextgen import aitextgen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Название загруженного текста Шекспира для тренировки\n",
    "file_name = \"Output.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:aitextgen.tokenizers:Saving aitextgen-vocab.json and aitextgen-merges.txt to the current directory. You will need both files to build the GPT2Tokenizer.\n"
     ]
    }
   ],
   "source": [
    "# Обучаем кастомный токенизатор BPE на загруженном тексте\n",
    "# Это сохранит два файла: aitextgen-vocab.json и aitextgen-merges.txt,\n",
    "# которые необходимы для восстановления токенизатора.\n",
    "train_tokenizer(file_name)\n",
    "vocab_file = \"aitextgen-vocab.json\"\n",
    "merges_file = \"aitextgen-merges.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT2ConfigCPU - это мини-вариант GPT-2, оптимизированный для обучения ЦП\n",
    "# например количество входных токенов здесь составляет 64 против 1024 для базового GPT-2.\n",
    "config = GPT2ConfigCPU()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:aitextgen:Constructing GPT-2 model from provided config.\n",
      "INFO:aitextgen:Using a custom tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Создание экземпляра aitextgen с использованием созданного токенизатора и конфигурации\n",
    "ai = aitextgen(vocab_file=vocab_file, merges_file=merges_file, config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "193a4d00059143a3a290b26efddf80e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, layout=Layout(flex='2'), max=1.0), HTML(value='')), layout=Layout(disp…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:aitextgen.TokenDataset:Encoding 1 sets of tokens from Output.txt.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Вы можете создавать наборы данных для обучения, создавая TokenDatasets,\n",
    "# который автоматически обрабатывает набор данных подходящего размера.\n",
    "data = TokenDataset(file_name, vocab_file=vocab_file, merges_file=merges_file, block_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:aitextgen:pytorch_model.bin already exists in /trained_model and will be overwritten!\n",
      "GPU available: False, used: False\n",
      "INFO:lightning:GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "INFO:lightning:TPU available: False, using: 0 TPU cores\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf9019e79b8f4992b34b5df2dcc0777c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, layout=Layout(flex='2'), max=5000.0), HTML(value='')), layout=Layout(d…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1,000 steps reached: saving model to /trained_model\u001b[0m\n",
      "\u001b[1m1,000 steps reached: generating sample texts.\u001b[0m\n",
      "==========\n",
      "ного в вот не не и в пать на в и не ша в ру и не ме пок обали и но в у в мом на в с в в о в у на стить о это но на лонили за а и очень в лах и его и с н\n",
      "==========\n",
      "\u001b[1m2,000 steps reached: saving model to /trained_model\u001b[0m\n",
      "\u001b[1m2,000 steps reached: generating sample texts.\u001b[0m\n",
      "==========\n",
      " с в бей на все все вот не не не не не очень не не на нас не не не не у все из кела и раз разом и мельных лелелой на том и в этом на его всё на фринам в суд не это не не кзы не\n",
      "==========\n",
      "\u001b[1m3,000 steps reached: saving model to /trained_model\u001b[0m\n",
      "\u001b[1m3,000 steps reached: generating sample texts.\u001b[0m\n",
      "==========\n",
      "ных гитевал и собопой и вот кодил в общем на них на недия с чежный дегалки и в году с ним в словам суды и был по гьетеко м этом в больницу по гагидорке на деле рублей в больницу\n",
      "==========\n",
      "\u001b[1m4,000 steps reached: saving model to /trained_model\u001b[0m\n",
      "\u001b[1m4,000 steps reached: generating sample texts.\u001b[0m\n",
      "==========\n",
      " поехал уже не забить на место на канале у него в ней такойющие во время и гезно гитаку и вы не было после день и об этом время в и очень был не в году на месте разомли в одном из гинский по делу с развили\n",
      "==========\n",
      "\u001b[1m5,000 steps reached: saving model to /trained_model\u001b[0m\n",
      "\u001b[1m5,000 steps reached: generating sample texts.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:aitextgen:Saving trained model pytorch_model.bin to /trained_model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========\n",
      " там в кухи и там они с собой теперь на себя в конце на территории и в полицию а и уже там всё просто бы не раз развло их не нечет в мат с собой уже было за дидивала в тагревойхомнитали о том что\n",
      "==========\n"
     ]
    }
   ],
   "source": [
    "# Обучите модель! Он будет сохранять pytorch_model.bin периодически и после завершения.\n",
    "# На MacBook Pro 2016 года это заняло ~ 25 минут.\n",
    "ai.train(data, batch_size=16, num_steps=5000, save_every = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mУбийство\u001b[0m за тысяча у насчинского кипочного на суткиу у вас в одном в общем по регионам а также в аэропорту разгана от нас но к этом году уже уже дан в суд на улице года это и дистакторы но не так а он выл\n",
      "==========\n",
      "\u001b[1mУбийство\u001b[0m кусы на мелька поехал в полицию за в больнице и не вышу по этом и и и это не только вот его словам суд с места не было же только поехали его в полицию на месте есть в пиных в полицию в больницу в году с ними не не\n",
      "==========\n",
      "\u001b[1mУбийство\u001b[0m он с его разненило это время в в том и на муловом леду мутинку а на улице за это их лето в суд но это его в в подмосковье с ними было это на улице за ним уходит в маклихом в том\n",
      "==========\n",
      "\u001b[1mУбийство\u001b[0m но с тру за морокер и это это не очень разные сет в бамарии а у неё а в этом в москву и все у него в этом и сетененом кимых в в логе аеева но не сталем из\n",
      "==========\n",
      "\u001b[1mУбийство\u001b[0m после того что уже так у того кто в итоге отходе мик и шемки в году с ним нетсетатах у него не нашли отобляки он в этом году кикборковского лобши но и это его в судно не очень\n"
     ]
    }
   ],
   "source": [
    "# генерируем текст!\n",
    "ai.generate(5, batch_size=15, max_length=1024, top_p=0.5, temperature=2.3, prompt=\"Убийство\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
